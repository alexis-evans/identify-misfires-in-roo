{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Functions for preprocessing text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str): return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Loading and combining datasets\n",
    "df_simple = df[['First_prompt', 'First_response', 'First_label']].copy()\n",
    "df_labeled = df_simple[df_simple['First_label'].notna()]\n",
    "csv_files = glob.glob('imported response data/*.csv')\n",
    "df_combined = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "df_combined = df_combined.rename(columns={\n",
    "    'Prompt ': 'First_prompt',\n",
    "    'Response ': 'First_response',\n",
    "    'Response Category (FP, FN, TP, TN)': 'First_label'\n",
    "})\n",
    "df_labeled = pd.concat([df_labeled, df_combined], ignore_index=True)\n",
    "df_labeled.shape\n",
    "# Cleaning text\n",
    "df_labeled['cleaned_prompt'] = df_labeled['First_prompt'].apply(preprocess_text)\n",
    "df_labeled['cleaned_response'] = df_labeled['First_response'].apply(preprocess_text)\n",
    "\n",
    "# Embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "prompt_embeddings = np.vstack([get_bert_embeddings(prompt) for prompt in df_labeled['cleaned_prompt']])\n",
    "response_embeddings = np.vstack([get_bert_embeddings(response) for response in df_labeled['cleaned_response']])\n",
    "combined_embeddings = prompt_embeddings + response_embeddings\n",
    "# Handling labels\n",
    "df_labeled['First_label'] = df_labeled['First_label'].fillna('FN')  # Default missing labels to FN\n",
    "df_encoded = pd.get_dummies(df_labeled, columns=['First_label'], prefix='First_label')\n",
    "\n",
    "# SVM Model Training\n",
    "df_encoded['combined_text'] = df_labeled['cleaned_prompt'] + \" \" + df_labeled['cleaned_response']\n",
    "y = df_labeled['First_label']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embeddings, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(\"Classification Report for SVM:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "#SVM tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on Test Data\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# LSTM Model Training\n",
    "X = combined_embeddings\n",
    "y = df_encoded[['First_label_FN', 'First_label_FP', 'First_label_TN', 'First_label_TP']]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X).reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "loss, accuracy = lstm_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"LSTM Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "#Random Forest\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embeddings, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "#random forest tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the grid search\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and evaluate\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate using the best model\n",
    "y_pred_best = best_rf_model.predict(X_test)\n",
    "print(f\"Accuracy (Best Model): {accuracy_score(y_test, y_pred_best)}\")\n",
    "print(\"Classification Report (Best Model):\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=label_encoder.classes_))\n",
    "#Logistic Regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embeddings, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "logreg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
